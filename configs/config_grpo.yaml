model:
  base_model: "model_exp/qwen8b_sft_en"
  max_seq_length: 8192
  lora_rank: 64
  gpu_memory_utilization: 0.5
  load_in_4bit: false
  fast_inference: true

dataset:
  data_dir: "data/data_en"
  dataset_type: "grpo"
  quantile_filter: 0.9

grpo:
  temperature: 0.6
  learning_rate: 5e-6
  weight_decay: 0.001
  warmup_ratio: 0.1
  lr_scheduler_type: "linear"
  optim: "adamw_8bit"
  batch_size: 1
  grad_acc_steps: 1
  num_generations: 4

training:
  num_epochs: 1
  max_steps: 1000
  save_steps: 50
  output_dir: "outputs"

sampling:
  min_p: 0.1
  top_p: 0.95
  top_k: 20
  seed: 3407

data_path: "./data/train.json"
model_name: "unsloth/Qwen3-8B"
max_seq_length: 8192
lora_rank: 256
gpu_memory_utilization: 0.6

seed: 42
report_to: "wandb"

training:
  batch_size: 8
  gradient_accumulation_steps: 1
  warmup_steps: 10
  epochs: 1
  learning_rate: 0.0002
  logging_steps: 5
  optim: "adamw_8bit"
  weight_decay: 0.001
  lr_scheduler_type: "linear"
  logging_dir: "./logs"

output_dir: "./model_exp"
output_name: "model_sft"
